{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 5 exercise\n",
    "\n",
    "All the warnings and fixes are commented in the actual code in the cells.\n",
    "\n",
    "## Step 1. Install Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-12T19:14:34.193921Z",
     "iopub.status.busy": "2025-02-12T19:14:34.193645Z",
     "iopub.status.idle": "2025-02-12T19:15:02.492054Z",
     "shell.execute_reply": "2025-02-12T19:15:02.490904Z",
     "shell.execute_reply.started": "2025-02-12T19:14:34.193891Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rouge_score in c:\\users\\saku\\documents\\ohjelmointi\\python\\finetuning_llms_course\\week4_exercises\\.venv\\lib\\site-packages (0.1.2)\n",
      "Requirement already satisfied: absl-py in c:\\users\\saku\\documents\\ohjelmointi\\python\\finetuning_llms_course\\week4_exercises\\.venv\\lib\\site-packages (from rouge_score) (2.1.0)\n",
      "Requirement already satisfied: nltk in c:\\users\\saku\\documents\\ohjelmointi\\python\\finetuning_llms_course\\week4_exercises\\.venv\\lib\\site-packages (from rouge_score) (3.9.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\saku\\documents\\ohjelmointi\\python\\finetuning_llms_course\\week4_exercises\\.venv\\lib\\site-packages (from rouge_score) (2.2.2)\n",
      "Requirement already satisfied: six>=1.14.0 in c:\\users\\saku\\documents\\ohjelmointi\\python\\finetuning_llms_course\\week4_exercises\\.venv\\lib\\site-packages (from rouge_score) (1.17.0)\n",
      "Requirement already satisfied: joblib in c:\\users\\saku\\documents\\ohjelmointi\\python\\finetuning_llms_course\\week4_exercises\\.venv\\lib\\site-packages (from nltk->rouge_score) (1.4.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\saku\\documents\\ohjelmointi\\python\\finetuning_llms_course\\week4_exercises\\.venv\\lib\\site-packages (from nltk->rouge_score) (4.67.1)\n",
      "Requirement already satisfied: click in c:\\users\\saku\\documents\\ohjelmointi\\python\\finetuning_llms_course\\week4_exercises\\.venv\\lib\\site-packages (from nltk->rouge_score) (8.1.8)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\saku\\documents\\ohjelmointi\\python\\finetuning_llms_course\\week4_exercises\\.venv\\lib\\site-packages (from nltk->rouge_score) (2024.11.6)\n",
      "Requirement already satisfied: colorama in c:\\users\\saku\\documents\\ohjelmointi\\python\\finetuning_llms_course\\week4_exercises\\.venv\\lib\\site-packages (from click->nltk->rouge_score) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install -q -U bitsandbytes transformers peft accelerate datasets scipy einops evaluate trl\n",
    "!pip install rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-12T19:16:07.904539Z",
     "iopub.status.busy": "2025-02-12T19:16:07.904127Z",
     "iopub.status.idle": "2025-02-12T19:16:07.915106Z",
     "shell.execute_reply": "2025-02-12T19:16:07.912895Z",
     "shell.execute_reply.started": "2025-02-12T19:16:07.904507Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BitsAndBytes version: 0.45.2\n",
      "Transformers version: 4.48.3\n",
      "PEFT version: 0.14.0\n",
      "Accelerate version: 1.3.0\n",
      "Datasets version: 3.2.0\n",
      "Scipy version: 1.15.1\n",
      "Einops version: 0.8.1\n",
      "Evaluate version: 0.4.3\n",
      "TRL version: 0.14.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import bitsandbytes\n",
    "import transformers\n",
    "import peft\n",
    "import accelerate\n",
    "import datasets\n",
    "import scipy\n",
    "import einops\n",
    "import evaluate\n",
    "import trl\n",
    "import rouge_score\n",
    "\n",
    "print(\"BitsAndBytes version:\", bitsandbytes.__version__)\n",
    "print(\"Transformers version:\", transformers.__version__)\n",
    "print(\"PEFT version:\", peft.__version__)\n",
    "print(\"Accelerate version:\", accelerate.__version__)\n",
    "print(\"Datasets version:\", datasets.__version__)\n",
    "print(\"Scipy version:\", scipy.__version__)\n",
    "print(\"Einops version:\", einops.__version__)\n",
    "print(\"Evaluate version:\", evaluate.__version__)\n",
    "print(\"TRL version:\", trl.__version__)\n",
    "# Rouge score does not have __version__, commented out\n",
    "# print(\"Rouge Score version:\", rouge_score.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-12T19:16:26.960143Z",
     "iopub.status.busy": "2025-02-12T19:16:26.959743Z",
     "iopub.status.idle": "2025-02-12T19:16:26.964227Z",
     "shell.execute_reply": "2025-02-12T19:16:26.963402Z",
     "shell.execute_reply.started": "2025-02-12T19:16:26.960112Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#import os\n",
    "# disable Weights and Biases\n",
    "# OUTDATED\n",
    "# os.environ['WANDB_DISABLED']=\"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-12T19:16:39.339259Z",
     "iopub.status.busy": "2025-02-12T19:16:39.338891Z",
     "iopub.status.idle": "2025-02-12T19:16:40.677785Z",
     "shell.execute_reply": "2025-02-12T19:16:40.677151Z",
     "shell.execute_reply.started": "2025-02-12T19:16:39.339230Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    GenerationConfig\n",
    ")\n",
    "from tqdm import tqdm\n",
    "from trl import SFTTrainer\n",
    "import torch\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from huggingface_hub import login\n",
    "\n",
    "# from kaggle_secrets import UserSecretsClient\n",
    "# user_secrets = UserSecretsClient()\n",
    "# HF_HUB_TOKEN = user_secrets.get_secret(\"HF_TOKEN\")\n",
    "\n",
    "import os\n",
    "HF_HUB_TOKEN = os.environ.get(\"HF_HUB_TOKEN\")\n",
    "\n",
    "login(token=HF_HUB_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-12T19:17:22.303844Z",
     "iopub.status.busy": "2025-02-12T19:17:22.303528Z",
     "iopub.status.idle": "2025-02-12T19:17:22.308416Z",
     "shell.execute_reply": "2025-02-12T19:17:22.307637Z",
     "shell.execute_reply.started": "2025-02-12T19:17:22.303820Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from pynvml import *\n",
    "\n",
    "def print_gpu_utilization():\n",
    "    nvmlInit()\n",
    "    handle = nvmlDeviceGetHandleByIndex(0)\n",
    "    info = nvmlDeviceGetMemoryInfo(handle)\n",
    "    print(f\"GPU memory occupied: {info.used//1024**2} MB.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2.\n",
    "Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-12T19:17:32.062715Z",
     "iopub.status.busy": "2025-02-12T19:17:32.062398Z",
     "iopub.status.idle": "2025-02-12T19:17:34.838098Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "        num_rows: 1999\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "        num_rows: 499\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "        num_rows: 499\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://huggingface.co/datasets/neil-code/dialogsum-test\n",
    "huggingface_dataset_name = \"neil-code/dialogsum-test\"\n",
    "dataset = load_dataset(huggingface_dataset_name)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-12T19:17:39.482857Z",
     "iopub.status.busy": "2025-02-12T19:17:39.482561Z",
     "iopub.status.idle": "2025-02-12T19:17:39.488771Z",
     "shell.execute_reply": "2025-02-12T19:17:39.487731Z",
     "shell.execute_reply.started": "2025-02-12T19:17:39.482833Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'train_0',\n",
       " 'dialogue': \"#Person1#: Hi, Mr. Smith. I'm Doctor Hawkins. Why are you here today?\\n#Person2#: I found it would be a good idea to get a check-up.\\n#Person1#: Yes, well, you haven't had one for 5 years. You should have one every year.\\n#Person2#: I know. I figure as long as there is nothing wrong, why go see the doctor?\\n#Person1#: Well, the best way to avoid serious illnesses is to find out about them early. So try to come at least once a year for your own good.\\n#Person2#: Ok.\\n#Person1#: Let me see here. Your eyes and ears look fine. Take a deep breath, please. Do you smoke, Mr. Smith?\\n#Person2#: Yes.\\n#Person1#: Smoking is the leading cause of lung cancer and heart disease, you know. You really should quit.\\n#Person2#: I've tried hundreds of times, but I just can't seem to kick the habit.\\n#Person1#: Well, we have classes and some medications that might help. I'll give you more information before you leave.\\n#Person2#: Ok, thanks doctor.\",\n",
       " 'summary': \"Mr. Smith's getting a check-up, and Doctor Hawkins advises him to have one every year. Hawkins'll give some information about their classes and medications to help Mr. Smith quit smoking.\",\n",
       " 'topic': 'get a check-up'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. Create a bitsandbytes configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-12T19:17:59.768302Z",
     "iopub.status.busy": "2025-02-12T19:17:59.767975Z",
     "iopub.status.idle": "2025-02-12T19:17:59.773745Z",
     "shell.execute_reply": "2025-02-12T19:17:59.772894Z",
     "shell.execute_reply.started": "2025-02-12T19:17:59.768280Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "compute_dtype = getattr(torch, \"float16\")\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type='nf4',\n",
    "        bnb_4bit_compute_dtype=compute_dtype,\n",
    "        bnb_4bit_use_double_quant=False,\n",
    "    )\n",
    "device_map = {\"\": 0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4. Load the Llama 1.1B as required for finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-12T19:18:42.120212Z",
     "iopub.status.busy": "2025-02-12T19:18:42.119851Z",
     "iopub.status.idle": "2025-02-12T19:19:37.627418Z",
     "shell.execute_reply": "2025-02-12T19:19:37.626530Z",
     "shell.execute_reply.started": "2025-02-12T19:18:42.120184Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_name='TinyLlama/TinyLlama-1.1B-Chat-v1.0'\n",
    "original_model = AutoModelForCausalLM.from_pretrained(model_name, \n",
    "                                                      device_map=device_map,\n",
    "                                                      quantization_config=bnb_config,\n",
    "                                                      trust_remote_code=True,\n",
    "                                                      token=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5. Tokenization\n",
    "The default tokenizer is loaded and applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-12T19:20:11.382285Z",
     "iopub.status.busy": "2025-02-12T19:20:11.381905Z",
     "iopub.status.idle": "2025-02-12T19:20:12.783152Z",
     "shell.execute_reply": "2025-02-12T19:20:12.782460Z",
     "shell.execute_reply.started": "2025-02-12T19:20:11.382251Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# https://ai.stackexchange.com/questions/41485/while-fine-tuning-a-decoder-only-llm-like-llama-on-chat-dataset-what-kind-of-pa\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name,trust_remote_code=True,padding_side=\"left\",add_eos_token=True,add_bos_token=True,use_fast=False)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-12T19:20:18.530135Z",
     "iopub.status.busy": "2025-02-12T19:20:18.529763Z",
     "iopub.status.idle": "2025-02-12T19:20:18.535303Z",
     "shell.execute_reply": "2025-02-12T19:20:18.534467Z",
     "shell.execute_reply.started": "2025-02-12T19:20:18.530105Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory occupied: 3270 MB.\n"
     ]
    }
   ],
   "source": [
    "print_gpu_utilization()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-12T19:20:30.081096Z",
     "iopub.status.busy": "2025-02-12T19:20:30.080754Z",
     "iopub.status.idle": "2025-02-12T19:20:30.323414Z",
     "shell.execute_reply": "2025-02-12T19:20:30.322477Z",
     "shell.execute_reply.started": "2025-02-12T19:20:30.081068Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "eval_tokenizer = AutoTokenizer.from_pretrained(model_name, add_bos_token=True, trust_remote_code=True, use_fast=False)\n",
    "eval_tokenizer.pad_token = eval_tokenizer.eos_token\n",
    "\n",
    "def gen(model,p, maxlen=100, sample=True):\n",
    "    toks = eval_tokenizer(p, return_tensors=\"pt\")\n",
    "    res = model.generate(**toks.to(\"cuda\"), max_new_tokens=maxlen, do_sample=sample,num_return_sequences=1,temperature=0.1,num_beams=1,top_p=0.95,).to('cpu')\n",
    "    return eval_tokenizer.batch_decode(res,skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6. Test\n",
    "Test the model with zero shot tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-12T19:20:43.137957Z",
     "iopub.status.busy": "2025-02-12T19:20:43.137647Z",
     "iopub.status.idle": "2025-02-12T19:20:46.868608Z",
     "shell.execute_reply": "2025-02-12T19:20:46.867790Z",
     "shell.execute_reply.started": "2025-02-12T19:20:43.137932Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "Instruct: Summarize the following conversation.\n",
      "#Person1#: Happy Birthday, this is for you, Brian.\n",
      "#Person2#: I'm so happy you remember, please come in and enjoy the party. Everyone's here, I'm sure you have a good time.\n",
      "#Person1#: Brian, may I have a pleasure to have a dance with you?\n",
      "#Person2#: Ok.\n",
      "#Person1#: This is really wonderful party.\n",
      "#Person2#: Yes, you are always popular with everyone. and you look very pretty today.\n",
      "#Person1#: Thanks, that's very kind of you to say. I hope my necklace goes with my dress, and they both make me look good I feel.\n",
      "#Person2#: You look great, you are absolutely glowing.\n",
      "#Person1#: Thanks, this is a fine party. We should have a drink together to celebrate your birthday\n",
      "Output:\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# attends Brian's birthday party. Brian thinks #Person1# looks great and charming.\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - ZERO SHOT:\n",
      "Brian: Happy Birthday, this is for you, Brian.\n",
      "Person2: I'm so happy you remember, please come in and enjoy the party. Everyone's here, I'm sure you have a good time.\n",
      "Person1: Brian, may I have a pleasure to have a dance with you?\n",
      "Person2: Ok.\n",
      "Person1: This is really wonderful party.\n",
      "Person2: Yes, you are always popular with everyone. and you\n",
      "CPU times: total: 547 ms\n",
      "Wall time: 2.39 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from transformers import set_seed\n",
    "seed = 42\n",
    "set_seed(seed)\n",
    "\n",
    "index = 10\n",
    "\n",
    "prompt = dataset['test'][index]['dialogue']\n",
    "summary = dataset['test'][index]['summary']\n",
    "\n",
    "formatted_prompt = f\"Instruct: Summarize the following conversation.\\n{prompt}\\nOutput:\\n\"\n",
    "res = gen(original_model,formatted_prompt,100,)\n",
    "#print(res[0])\n",
    "output = res[0].split('Output:\\n')[1]\n",
    "\n",
    "dash_line = '-'.join('' for x in range(100))\n",
    "print(dash_line)\n",
    "print(f'INPUT PROMPT:\\n{formatted_prompt}')\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\n",
    "print(dash_line)\n",
    "print(f'MODEL GENERATION - ZERO SHOT:\\n{output}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-12T19:22:25.657151Z",
     "iopub.status.busy": "2025-02-12T19:22:25.656773Z",
     "iopub.status.idle": "2025-02-12T19:22:25.662191Z",
     "shell.execute_reply": "2025-02-12T19:22:25.661369Z",
     "shell.execute_reply.started": "2025-02-12T19:22:25.657120Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def create_prompt_formats(sample):\n",
    "    \"\"\"\n",
    "    Format various fields of the sample ('instruction','output')\n",
    "    Then concatenate them using two newline characters \n",
    "    :param sample: Sample dictionnary\n",
    "    \"\"\"\n",
    "    INTRO_BLURB = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n",
    "    INSTRUCTION_KEY = \"### Instruct: Summarize the below conversation.\"\n",
    "    RESPONSE_KEY = \"### Output:\"\n",
    "    END_KEY = \"### End\"\n",
    "    \n",
    "    blurb = f\"\\n{INTRO_BLURB}\"\n",
    "    instruction = f\"{INSTRUCTION_KEY}\"\n",
    "    input_context = f\"{sample['dialogue']}\" if sample[\"dialogue\"] else None\n",
    "    response = f\"{RESPONSE_KEY}\\n{sample['summary']}\"\n",
    "    end = f\"{END_KEY}\"\n",
    "    \n",
    "    parts = [part for part in [blurb, instruction, input_context, response, end] if part]\n",
    "\n",
    "    formatted_prompt = \"\\n\\n\".join(parts)\n",
    "    sample[\"text\"] = formatted_prompt\n",
    "\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-12T19:22:43.747497Z",
     "iopub.status.busy": "2025-02-12T19:22:43.747180Z",
     "iopub.status.idle": "2025-02-12T19:22:43.752451Z",
     "shell.execute_reply": "2025-02-12T19:22:43.751533Z",
     "shell.execute_reply.started": "2025-02-12T19:22:43.747471Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# SOURCE https://github.com/databrickslabs/dolly/blob/master/training/trainer.py\n",
    "def get_max_length(model):\n",
    "    conf = model.config\n",
    "    max_length = None\n",
    "    for length_setting in [\"n_positions\", \"max_position_embeddings\", \"seq_length\"]:\n",
    "        max_length = getattr(model.config, length_setting, None)\n",
    "        if max_length:\n",
    "            print(f\"Found max lenth: {max_length}\")\n",
    "            break\n",
    "    if not max_length:\n",
    "        max_length = 1024\n",
    "        print(f\"Using default max length: {max_length}\")\n",
    "    return max_length\n",
    "\n",
    "\n",
    "def preprocess_batch(batch, tokenizer, max_length):\n",
    "    \"\"\"\n",
    "    Tokenizing a batch\n",
    "    \"\"\"\n",
    "    return tokenizer(\n",
    "        batch[\"text\"],\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7. Preprocess the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-12T19:23:01.513148Z",
     "iopub.status.busy": "2025-02-12T19:23:01.512799Z",
     "iopub.status.idle": "2025-02-12T19:23:01.518332Z",
     "shell.execute_reply": "2025-02-12T19:23:01.517369Z",
     "shell.execute_reply.started": "2025-02-12T19:23:01.513119Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "# SOURCE https://github.com/databrickslabs/dolly/blob/master/training/trainer.py\n",
    "def preprocess_dataset(tokenizer: AutoTokenizer, max_length: int,seed, dataset):\n",
    "    \"\"\"Format & tokenize it so it is ready for training\n",
    "    :param tokenizer (AutoTokenizer): Model Tokenizer\n",
    "    :param max_length (int): Maximum number of tokens to emit from tokenizer\n",
    "    \"\"\"\n",
    "    \n",
    "    # Add prompt to each sample\n",
    "    print(\"Preprocessing dataset...\")\n",
    "    dataset = dataset.map(create_prompt_formats)#, batched=True)\n",
    "    \n",
    "    _preprocessing_function = partial(preprocess_batch, max_length=max_length, tokenizer=tokenizer)\n",
    "    dataset = dataset.map(\n",
    "        _preprocessing_function,\n",
    "        batched=True,\n",
    "        remove_columns=['id', 'topic', 'dialogue', 'summary'],\n",
    "    )\n",
    "\n",
    "    # Filter out samples that have input_ids exceeding max_length\n",
    "    dataset = dataset.filter(lambda sample: len(sample[\"input_ids\"]) < max_length)\n",
    "    \n",
    "    # Shuffle dataset\n",
    "    dataset = dataset.shuffle(seed=seed)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-12T19:23:05.890047Z",
     "iopub.status.busy": "2025-02-12T19:23:05.889722Z",
     "iopub.status.idle": "2025-02-12T19:23:05.894549Z",
     "shell.execute_reply": "2025-02-12T19:23:05.893556Z",
     "shell.execute_reply.started": "2025-02-12T19:23:05.890019Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory occupied: 3293 MB.\n"
     ]
    }
   ],
   "source": [
    "print_gpu_utilization()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-12T19:23:13.724605Z",
     "iopub.status.busy": "2025-02-12T19:23:13.724295Z",
     "iopub.status.idle": "2025-02-12T19:23:18.713399Z",
     "shell.execute_reply": "2025-02-12T19:23:18.712722Z",
     "shell.execute_reply.started": "2025-02-12T19:23:13.724569Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found max lenth: 2048\n",
      "2048\n",
      "Preprocessing dataset...\n",
      "Preprocessing dataset...\n",
      "Shapes of the datasets:\n",
      "Training: (1999, 3)\n",
      "Validation: (499, 3)\n",
      "Dataset({\n",
      "    features: ['text', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 1999\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# ## Pre-process dataset\n",
    "max_length = get_max_length(original_model)\n",
    "print(max_length)\n",
    "\n",
    "train_dataset = preprocess_dataset(tokenizer, max_length,seed, dataset['train'])\n",
    "eval_dataset = preprocess_dataset(tokenizer, max_length,seed, dataset['validation'])\n",
    "\n",
    "print(f\"Shapes of the datasets:\")\n",
    "print(f\"Training: {train_dataset.shape}\")\n",
    "print(f\"Validation: {eval_dataset.shape}\")\n",
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-12T19:23:25.272083Z",
     "iopub.status.busy": "2025-02-12T19:23:25.271752Z",
     "iopub.status.idle": "2025-02-12T19:23:25.278175Z",
     "shell.execute_reply": "2025-02-12T19:23:25.277257Z",
     "shell.execute_reply.started": "2025-02-12T19:23:25.272058Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable model parameters: 131164160\n",
      "all model parameters: 615606272\n",
      "percentage of trainable model parameters: 21.31%\n"
     ]
    }
   ],
   "source": [
    "def print_number_of_trainable_model_parameters(model):\n",
    "    trainable_model_params = 0\n",
    "    all_model_params = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_model_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_model_params += param.numel()\n",
    "    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n",
    "\n",
    "print(print_number_of_trainable_model_parameters(original_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-12T19:23:36.689915Z",
     "iopub.status.busy": "2025-02-12T19:23:36.689587Z",
     "iopub.status.idle": "2025-02-12T19:23:36.695481Z",
     "shell.execute_reply": "2025-02-12T19:23:36.694612Z",
     "shell.execute_reply.started": "2025-02-12T19:23:36.689891Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(32000, 2048)\n",
      "    (layers): ModuleList(\n",
      "      (0-21): 22 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear4bit(in_features=2048, out_features=5632, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=2048, out_features=5632, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=5632, out_features=2048, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=32000, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(original_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 8. Prepare the model for QLoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-12T19:23:50.562745Z",
     "iopub.status.busy": "2025-02-12T19:23:50.562442Z",
     "iopub.status.idle": "2025-02-12T19:23:50.719808Z",
     "shell.execute_reply": "2025-02-12T19:23:50.719142Z",
     "shell.execute_reply.started": "2025-02-12T19:23:50.562721Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=32, #Rank\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\n",
    "        'q_proj',\n",
    "        'k_proj',\n",
    "        'v_proj',\n",
    "        'dense'\n",
    "    ],\n",
    "    bias=\"none\",\n",
    "    lora_dropout=0.05,  # Conventional\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# 1 - Enabling gradient checkpointing to reduce memory usage during fine-tuning\n",
    "original_model.gradient_checkpointing_enable()\n",
    "\n",
    "# 2 - Using the prepare_model_for_kbit_training method from PEFT\n",
    "original_model = prepare_model_for_kbit_training(original_model)\n",
    "\n",
    "peft_model = get_peft_model(original_model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-12T19:23:59.034134Z",
     "iopub.status.busy": "2025-02-12T19:23:59.033788Z",
     "iopub.status.idle": "2025-02-12T19:23:59.041032Z",
     "shell.execute_reply": "2025-02-12T19:23:59.040142Z",
     "shell.execute_reply.started": "2025-02-12T19:23:59.034106Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable model parameters: 6127616\n",
      "all model parameters: 621733888\n",
      "percentage of trainable model parameters: 0.99%\n"
     ]
    }
   ],
   "source": [
    "print(print_number_of_trainable_model_parameters(peft_model))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9. Prepare the model for PEFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-12T19:33:03.547824Z",
     "iopub.status.busy": "2025-02-12T19:33:03.547486Z",
     "iopub.status.idle": "2025-02-12T19:33:03.584173Z",
     "shell.execute_reply": "2025-02-12T19:33:03.583302Z",
     "shell.execute_reply.started": "2025-02-12T19:33:03.547795Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "output_dir = './peft-dialogue-summary-training/final-checkpoint'\n",
    "import transformers\n",
    "\n",
    "peft_training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    warmup_steps=1,\n",
    "    per_device_train_batch_size=4,  # ✅ Increased for better GPU usage\n",
    "    gradient_accumulation_steps=2,  # ✅ Reduced for speed\n",
    "    max_steps=500, # As required\n",
    "    learning_rate=2e-4,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    logging_steps=50,  # ✅ Logs less often\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,  # ✅ Saves less often\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=50,  # ✅ Evaluates less often\n",
    "    do_eval=True,\n",
    "    gradient_checkpointing=False,  # ✅ Disabled for speed\n",
    "    fp16=True,  # ✅ Enables mixed precision\n",
    "    report_to=\"none\",\n",
    "    overwrite_output_dir=True,\n",
    "    group_by_length=True,\n",
    ")\n",
    "\n",
    "\n",
    "peft_model.config.use_cache = False\n",
    "\n",
    "# Old code, with warning\n",
    "peft_trainer = transformers.Trainer(\n",
    "    model=peft_model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    args=peft_training_args,\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "\n",
    "# # New code fixing warnings\n",
    "# peft_trainer = Trainer(\n",
    "#     model=peft_model,\n",
    "#     train_dataset=train_dataset,\n",
    "#     eval_dataset=eval_dataset,\n",
    "#     args=peft_training_args,\n",
    "#     data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    "#     processing_class=transformers.DefaultDataCollator,  # Explicitly set processing_class to avoid the warning\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10. Train PEFT adapter\n",
    "Train the model, required parameters set above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is needed to avoid use_reentrant warning\n",
    "import torch.utils.checkpoint\n",
    "\n",
    "torch.utils.checkpoint.use_reentrant = False  # This disables reentrant checkpointing globally\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-12T19:33:08.309851Z",
     "iopub.status.busy": "2025-02-12T19:33:08.309473Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\saku\\Documents\\Ohjelmointi\\Python\\finetuning_llms_course\\week4_exercises\\.venv\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [500/500 04:58, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.433800</td>\n",
       "      <td>1.332565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.256900</td>\n",
       "      <td>1.312666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.288100</td>\n",
       "      <td>1.283000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.267800</td>\n",
       "      <td>1.273433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.275500</td>\n",
       "      <td>1.272425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.237800</td>\n",
       "      <td>1.266185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>1.250500</td>\n",
       "      <td>1.262303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.233600</td>\n",
       "      <td>1.260834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>1.224300</td>\n",
       "      <td>1.260159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.232400</td>\n",
       "      <td>1.259176</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "c:\\Users\\saku\\Documents\\Ohjelmointi\\Python\\finetuning_llms_course\\week4_exercises\\.venv\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "c:\\Users\\saku\\Documents\\Ohjelmointi\\Python\\finetuning_llms_course\\week4_exercises\\.venv\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "c:\\Users\\saku\\Documents\\Ohjelmointi\\Python\\finetuning_llms_course\\week4_exercises\\.venv\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "c:\\Users\\saku\\Documents\\Ohjelmointi\\Python\\finetuning_llms_course\\week4_exercises\\.venv\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=500, training_loss=1.270070899963379, metrics={'train_runtime': 300.5747, 'train_samples_per_second': 13.308, 'train_steps_per_second': 1.663, 'total_flos': 7998465185280000.0, 'train_loss': 1.270070899963379, 'epoch': 2.0})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_training_args.device\n",
    "peft_trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concerning the warnings;\n",
    "Checkpoint warning can be manually removed and relates probably to the actual python version used with the other modules\n",
    "Torch.tokenizer warning is much trickier; it probably has origins inside torch and there were issues reported with\n",
    "this in the internet. \n",
    "However, after they don't affect the function of the notebook, I finally let them be..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 13. Save the model to HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "681a8d026e7445d285da4a2658ba6afd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\saku\\Documents\\Ohjelmointi\\Python\\finetuning_llms_course\\week4_exercises\\.venv\\lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\saku\\.cache\\huggingface\\hub\\models--Kelmeilia--llama1_1chat-dialogsum-finetuned. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf76259c051a4f20b97691fa567ad99e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/24.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb97ddd558ad45229f3881c6edc22792",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/Kelmeilia/llama1_1chat-dialogsum-finetuned/commit/7e04e3ab68e1307c9c6575b7c44f337be196cb85', commit_message='Upload tokenizer', commit_description='', oid='7e04e3ab68e1307c9c6575b7c44f337be196cb85', pr_url=None, repo_url=RepoUrl('https://huggingface.co/Kelmeilia/llama1_1chat-dialogsum-finetuned', endpoint='https://huggingface.co', repo_type='model', repo_id='Kelmeilia/llama1_1chat-dialogsum-finetuned'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Already logged in \n",
    "# from huggingface_hub import notebook_login\n",
    "# notebook_login()  # Enter your HF token\n",
    "\n",
    "# Upload Model & Tokenizer\n",
    "peft_model.push_to_hub(\"Kelmeilia/llama1_1chat-dialogsum-finetuned\")\n",
    "tokenizer.push_to_hub(\"Kelmeilia/llama1_1chat-dialogsum-finetuned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting finetuned Llama 1.1 model at HF: https://huggingface.co/Kelmeilia/llama1_1chat-dialogsum-finetuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory occupied: 11009 MB.\n",
      "GPU memory occupied: 3909 MB.\n"
     ]
    }
   ],
   "source": [
    "print_gpu_utilization()\n",
    "del original_model\n",
    "del peft_trainer\n",
    "torch.cuda.empty_cache()\n",
    "print_gpu_utilization()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\saku\\Documents\\Ohjelmointi\\Python\\finetuning_llms_course\\week4_exercises\\.venv\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "base_model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "base_model = AutoModelForCausalLM.from_pretrained(base_model_id, \n",
    "                                                      device_map='auto',\n",
    "                                                      quantization_config=bnb_config,\n",
    "                                                      trust_remote_code=True,\n",
    "                                                      use_auth_token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "eval_tokenizer = AutoTokenizer.from_pretrained(base_model_id, add_bos_token=True, trust_remote_code=True, use_fast=False)\n",
    "eval_tokenizer.pad_token = eval_tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "# Name needs update\n",
    "ft_model = PeftModel.from_pretrained(base_model, \"./peft-dialogue-summary-training/final-checkpoint/checkpoint-500\",torch_dtype=torch.float16,is_trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "Instruct: Summarize the following conversation.\n",
      "#Person1#: Happy Birthday, this is for you, Brian.\n",
      "#Person2#: I'm so happy you remember, please come in and enjoy the party. Everyone's here, I'm sure you have a good time.\n",
      "#Person1#: Brian, may I have a pleasure to have a dance with you?\n",
      "#Person2#: Ok.\n",
      "#Person1#: This is really wonderful party.\n",
      "#Person2#: Yes, you are always popular with everyone. and you look very pretty today.\n",
      "#Person1#: Thanks, that's very kind of you to say. I hope my necklace goes with my dress, and they both make me look good I feel.\n",
      "#Person2#: You look great, you are absolutely glowing.\n",
      "#Person1#: Thanks, this is a fine party. We should have a drink together to celebrate your birthday\n",
      "Output:\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# attends Brian's birthday party. Brian thinks #Person1# looks great and charming.\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "PEFT MODEL:\n",
      "#Person1# invites Brian to the party and thanks him for the birthday party.\n",
      "\n",
      "###### Step 4: End of the conversation.\n",
      "\n",
      "###### \n",
      "CPU times: total: 375 ms\n",
      "Wall time: 3.16 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from transformers import set_seed\n",
    "set_seed(seed)\n",
    "\n",
    "index = 10\n",
    "dialogue = dataset['test'][index]['dialogue']\n",
    "summary = dataset['test'][index]['summary']\n",
    "\n",
    "prompt = f\"Instruct: Summarize the following conversation.\\n{dialogue}\\nOutput:\\n\"\n",
    "\n",
    "peft_model_res = gen(ft_model,prompt,100,)\n",
    "peft_model_output = peft_model_res[0].split('Output:\\n')[1]\n",
    "#print(peft_model_output)\n",
    "prefix, success, result = peft_model_output.partition('#End')\n",
    "\n",
    "dash_line = '-'.join('' for x in range(100))\n",
    "print(dash_line)\n",
    "print(f'INPUT PROMPT:\\n{prompt}')\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\n",
    "print(dash_line)\n",
    "print(f'PEFT MODEL:\\n{prefix}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like working well..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "original_model = AutoModelForCausalLM.from_pretrained(base_model_id, \n",
    "                                                      device_map='auto',\n",
    "                                                      quantization_config=bnb_config,\n",
    "                                                      trust_remote_code=True,\n",
    "                                                      use_auth_token=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11. Evaluate the model qualitatively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>human_baseline_summaries</th>\n",
       "      <th>original_model_summaries</th>\n",
       "      <th>peft_model_summaries</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ms. Dawson helps #Person1# to write a memo to ...</td>\n",
       "      <td>#Person1#: Attention all staff... Effective im...</td>\n",
       "      <td>#Person1# dictates an intra-office memorandum ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In order to prevent employees from wasting tim...</td>\n",
       "      <td>#Person1#: Ms. Dawson, I need you to take a di...</td>\n",
       "      <td>#Person1# dictates an intra-office memorandum ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ms. Dawson takes a dictation for #Person1# abo...</td>\n",
       "      <td>#Person1#: Ms. Dawson, I need you to take a di...</td>\n",
       "      <td>#Person1# dictates an intra-office memorandum ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#Person2# arrives late because of traffic jam....</td>\n",
       "      <td>- The conversation between two people discussi...</td>\n",
       "      <td>#Person1# and #Person2# have a conversation ab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#Person2# decides to follow #Person1#'s sugges...</td>\n",
       "      <td>- The conversation between two people discussi...</td>\n",
       "      <td>#Person1# and #Person2# have a conversation ab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>#Person2# complains to #Person1# about the tra...</td>\n",
       "      <td>- The conversation between two people discussi...</td>\n",
       "      <td>#Person1# and #Person2# are discussing #Person...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>#Person1# tells Kate that Masha and Hero get d...</td>\n",
       "      <td>- The conversation between two friends, one of...</td>\n",
       "      <td>#Person1# and #Person2# talk about the divorce...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>#Person1# tells Kate that Masha and Hero are g...</td>\n",
       "      <td>- The conversation between two people, includi...</td>\n",
       "      <td>#Person1# and #Person2# talk about the divorce...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>#Person1# and Kate talk about the divorce betw...</td>\n",
       "      <td>- The conversation between two friends, one of...</td>\n",
       "      <td>#Person1# and #Person2# are discussing the div...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>#Person1# and Brian are at the birthday party ...</td>\n",
       "      <td>Brian: Happy Birthday, this is for you, Brian....</td>\n",
       "      <td>#Person1# invites Brian to the party and thank...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            human_baseline_summaries  \\\n",
       "0  Ms. Dawson helps #Person1# to write a memo to ...   \n",
       "1  In order to prevent employees from wasting tim...   \n",
       "2  Ms. Dawson takes a dictation for #Person1# abo...   \n",
       "3  #Person2# arrives late because of traffic jam....   \n",
       "4  #Person2# decides to follow #Person1#'s sugges...   \n",
       "5  #Person2# complains to #Person1# about the tra...   \n",
       "6  #Person1# tells Kate that Masha and Hero get d...   \n",
       "7  #Person1# tells Kate that Masha and Hero are g...   \n",
       "8  #Person1# and Kate talk about the divorce betw...   \n",
       "9  #Person1# and Brian are at the birthday party ...   \n",
       "\n",
       "                            original_model_summaries  \\\n",
       "0  #Person1#: Attention all staff... Effective im...   \n",
       "1  #Person1#: Ms. Dawson, I need you to take a di...   \n",
       "2  #Person1#: Ms. Dawson, I need you to take a di...   \n",
       "3  - The conversation between two people discussi...   \n",
       "4  - The conversation between two people discussi...   \n",
       "5  - The conversation between two people discussi...   \n",
       "6  - The conversation between two friends, one of...   \n",
       "7  - The conversation between two people, includi...   \n",
       "8  - The conversation between two friends, one of...   \n",
       "9  Brian: Happy Birthday, this is for you, Brian....   \n",
       "\n",
       "                                peft_model_summaries  \n",
       "0  #Person1# dictates an intra-office memorandum ...  \n",
       "1  #Person1# dictates an intra-office memorandum ...  \n",
       "2  #Person1# dictates an intra-office memorandum ...  \n",
       "3  #Person1# and #Person2# have a conversation ab...  \n",
       "4  #Person1# and #Person2# have a conversation ab...  \n",
       "5  #Person1# and #Person2# are discussing #Person...  \n",
       "6  #Person1# and #Person2# talk about the divorce...  \n",
       "7  #Person1# and #Person2# talk about the divorce...  \n",
       "8  #Person1# and #Person2# are discussing the div...  \n",
       "9  #Person1# invites Brian to the party and thank...  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dialogues = dataset['test'][0:10]['dialogue']\n",
    "human_baseline_summaries = dataset['test'][0:10]['summary']\n",
    "\n",
    "original_model_summaries = []\n",
    "instruct_model_summaries = []\n",
    "peft_model_summaries = []\n",
    "\n",
    "for idx, dialogue in enumerate(dialogues):\n",
    "    human_baseline_text_output = human_baseline_summaries[idx]\n",
    "    prompt = f\"Instruct: Summarize the following conversation.\\n{dialogue}\\nOutput:\\n\"\n",
    "    \n",
    "    original_model_res = gen(original_model,prompt,100,)\n",
    "    original_model_text_output = original_model_res[0].split('Output:\\n')[1]\n",
    "    \n",
    "    peft_model_res = gen(ft_model,prompt,100,)\n",
    "    peft_model_output = peft_model_res[0].split('Output:\\n')[1]\n",
    "    #print(peft_model_output)\n",
    "    peft_model_text_output, success, result = peft_model_output.partition('#End')\n",
    "    \n",
    "\n",
    "    original_model_summaries.append(original_model_text_output)\n",
    "    peft_model_summaries.append(peft_model_text_output)\n",
    "\n",
    "zipped_summaries = list(zip(human_baseline_summaries, original_model_summaries, peft_model_summaries))\n",
    " \n",
    "df = pd.DataFrame(zipped_summaries, columns = ['human_baseline_summaries', 'original_model_summaries', 'peft_model_summaries'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Already imported\n",
    "# !pip install rouge_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 12. Evaluate the model quantitatively (Rouge metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL MODEL:\n",
      "{'rouge1': np.float64(0.25753628055033617), 'rouge2': np.float64(0.06328586872065134), 'rougeL': np.float64(0.17921468341378305), 'rougeLsum': np.float64(0.18232591492825662)}\n",
      "PEFT MODEL:\n",
      "{'rouge1': np.float64(0.2953134333971185), 'rouge2': np.float64(0.06956296148156613), 'rougeL': np.float64(0.2323345694640548), 'rougeLsum': np.float64(0.2226705479325638)}\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "rouge = evaluate.load('rouge')\n",
    "\n",
    "original_model_results = rouge.compute(\n",
    "    predictions=original_model_summaries,\n",
    "    references=human_baseline_summaries[0:len(original_model_summaries)],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "\n",
    "peft_model_results = rouge.compute(\n",
    "    predictions=peft_model_summaries,\n",
    "    references=human_baseline_summaries[0:len(peft_model_summaries)],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "\n",
    "print('ORIGINAL MODEL:')\n",
    "print(original_model_results)\n",
    "print('PEFT MODEL:')\n",
    "print(peft_model_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absolute percentage improvement of PEFT MODEL over ORIGINAL MODEL\n",
      "rouge1: 3.78%\n",
      "rouge2: 0.63%\n",
      "rougeL: 5.31%\n",
      "rougeLsum: 4.03%\n"
     ]
    }
   ],
   "source": [
    "print(\"Absolute percentage improvement of PEFT MODEL over ORIGINAL MODEL\")\n",
    "\n",
    "improvement = (np.array(list(peft_model_results.values())) - np.array(list(original_model_results.values())))\n",
    "for key, value in zip(peft_model_results.keys(), improvement):\n",
    "    print(f'{key}: {value*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 14. Capture and document results\n",
    "\n",
    "The required files, including a local version with requirements are uploaded to github. Unfortunately I forgot that week 4 had no technical exercises, so the GH repo is named a bit misleadingly.<br>\n",
    "Github: https://github.com/SakuOrdrTab/week4_exercises<br>\n",
    "HuggingFace: https://huggingface.co/Kelmeilia/llama1_1chat-dialogsum-finetuned<br>"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
